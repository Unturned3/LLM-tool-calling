{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6667790",
   "metadata": {},
   "source": [
    "I am assuming we have access two jsonls\n",
    "- Student Outptuts\n",
    "    - `index`: Unique identifier for each question.\n",
    "    - `output`: The model's response to the question.\n",
    "\n",
    "- Hidden Test Set with the following fields:\n",
    "    - `index`: Unique identifier for each question.\n",
    "    - `task`: The name of the task (e.g., \"mmlu_med\").\n",
    "    - `prompt`: The question prompt presented to the model.\n",
    "    - `gold_answer`: The correct answer to the question.\n",
    "    - (Not needed)`meta`: Additional metadata about the question, including unique id in the dataset and other fields.\n",
    "\n",
    "For this grading logic, we assume we can get the task and other info by essentially grouping by `index` from the hidden test set and joining with the student outputs on `index`.\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4231eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any\n",
    "from grader import (\n",
    "    InfoBenchEvaluator,\n",
    "    GraphEvaluator,\n",
    "    MMLUEvaluator,\n",
    "    ResponseParser,\n",
    "    evaluate_single,\n",
    ")\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1ccf9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hidden_test(path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load hidden test JSONL file.\"\"\"\n",
    "    data = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_student_outputs(path: str) -> Dict[int, str]:\n",
    "    \"\"\"Load student outputs JSONL, return dict mapping index -> output.\"\"\"\n",
    "    outputs = {}\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                item = json.loads(line)\n",
    "                outputs[item[\"index\"]] = item.get(\"output\", \"\")\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "781b93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(data: list, path: str):\n",
    "    \"\"\"Save list of dicts to JSONL.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "\n",
    "def save_json(data: dict, path: str):\n",
    "    \"\"\"Save dict to JSON.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c24b5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTe: Remove later helper for printing metrics\n",
    "\n",
    "\n",
    "def print_metrics(metrics: dict):\n",
    "    \"\"\"Print metrics summary.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"RESULTS: {metrics['student_id']}\")\n",
    "    print(\"=\" * 50)\n",
    "    for task, m in metrics[\"task_metrics\"].items():\n",
    "        print(f\"{task:12s}: {m['accuracy']:.4f} ({m['count']} examples)\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'OVERALL':12s}: {metrics['overall_accuracy']:.4f}\")\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f708f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# METRICS\n",
    "# ============================================================================\n",
    "def calculate_metrics(results: list, student_id: str) -> dict:\n",
    "    \"\"\"Calculate task-wise and overall metrics.\"\"\"\n",
    "    task_scores = {\"mmlu_med\": [], \"graph\": [], \"infobench\": []}\n",
    "\n",
    "    for r in results:\n",
    "        task = r[\"task\"]\n",
    "        if task in task_scores:\n",
    "            task_scores[task].append(r[\"score\"])\n",
    "\n",
    "    metrics = {\n",
    "        \"student_id\": student_id,\n",
    "        \"total_examples\": len(results),\n",
    "        \"task_metrics\": {},\n",
    "        \"overall_accuracy\": 0.0,\n",
    "    }\n",
    "\n",
    "    all_scores = []\n",
    "    for task, scores in task_scores.items():\n",
    "        if scores:\n",
    "            metrics[\"task_metrics\"][task] = {\n",
    "                \"count\": len(scores),\n",
    "                \"accuracy\": sum(scores) / len(scores),\n",
    "                \"total_score\": sum(scores),\n",
    "            }\n",
    "            all_scores.extend(scores)\n",
    "\n",
    "    if all_scores:\n",
    "        metrics[\"overall_accuracy\"] = sum(all_scores) / len(all_scores)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d2dac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(\n",
    "    hidden_test: list, student_outputs: dict, infobench_evaluator: InfoBenchEvaluator\n",
    ") -> list:\n",
    "    \"\"\"Run evaluation on all test items.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for idx, test_item in enumerate(tqdm(hidden_test, desc=\"Evaluating\")):\n",
    "        index = test_item[\"index\"]\n",
    "        student_response = student_outputs.get(index, \"\")\n",
    "        result = evaluate_single(idx, test_item, student_response, infobench_evaluator)\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7924f4",
   "metadata": {},
   "source": [
    "# RUN EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48d37351",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea28f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform simulation summary into the .jsonl format required here\n",
    "\n",
    "run_name = \"run15\"\n",
    "sim_summary_path = f\"../request/{run_name}/simulation_summary.json\"\n",
    "output_path = f\"student_outputs_{run_name}.jsonl\"\n",
    "\n",
    "with open(sim_summary_path, \"r\") as f:\n",
    "    sim_summary = json.load(f)\n",
    "\n",
    "results = sim_summary[\"results\"]\n",
    "\n",
    "# Each item in `results` is a dict: {'prompt_idxs': [int, ...], 'response': {'choices': [{'index': int, 'text': str}, ...]}}\n",
    "# The .jsonl format we need here has each line as: {'index': int, 'output': str}\n",
    "\n",
    "student_outputs = {}\n",
    "for item in results:\n",
    "    prompt_idxs = item[\"prompt_idxs\"]\n",
    "    response = item[\"response\"]\n",
    "    choices = response[\"choices\"] if response is not None else []\n",
    "    if not choices:\n",
    "        for overall_index in prompt_idxs:\n",
    "            student_outputs[overall_index] = \"Error: no response\"\n",
    "        continue\n",
    "    for choice in choices:\n",
    "        choice_index = choice[\"index\"]\n",
    "        overall_index = prompt_idxs[choice_index]\n",
    "        student_outputs[overall_index] = choice[\"text\"]\n",
    "\n",
    "# save student outputs to a .jsonl fil\n",
    "with open(output_path, \"w\") as f:\n",
    "    for index, output in student_outputs.items():\n",
    "        json_line = {\"index\": index, \"output\": output}\n",
    "        f.write(json.dumps(json_line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e71859fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Hidden test size: 300\n",
      "Student outputs: 300\n"
     ]
    }
   ],
   "source": [
    "# === Configuration ===\n",
    "HIDDEN_TEST_PATH = \"combined_dataset_full.jsonl\"\n",
    "STUDENT_OUTPUT_PATH = f\"student_outputs_{run_name}.jsonl\"\n",
    "OUTPUT_DIR = f\"./eval_results_{run_name}\"\n",
    "STUDENT_ID = \"test_student\"\n",
    "EVAL_MODEL = \"gpt-5-nano-2025-08-07\"\n",
    "\n",
    "if not openai_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set.\")\n",
    "# === Load data ===\n",
    "print(\"Loading data...\")\n",
    "hidden_test = load_hidden_test(HIDDEN_TEST_PATH)\n",
    "student_outputs = load_student_outputs(STUDENT_OUTPUT_PATH)\n",
    "\n",
    "print(f\"Hidden test size: {len(hidden_test)}\")\n",
    "print(f\"Student outputs: {len(student_outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be80fc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing InfoBench evaluator...\n"
     ]
    }
   ],
   "source": [
    "# # === Initialize InfoBench Evaluator ===\n",
    "print(\"\\nInitializing InfoBench evaluator...\")\n",
    "infobench_evaluator = InfoBenchEvaluator(openai_key, EVAL_MODEL)\n",
    "\n",
    "# print(\"Verifying OpenAI connection...\")\n",
    "# if not infobench_evaluator.verify_connection():\n",
    "#     raise RuntimeError(\"OpenAI connection failed - cannot proceed\")\n",
    "# print(\"OpenAI connection verified ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c98ca0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: test_student\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 300/300 [00:00<00:00, 85627.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# === Run Evaluation ===\n",
    "print(f\"\\nEvaluating: {STUDENT_ID}\")\n",
    "print(\"-\" * 50)\n",
    "results = run_eval(hidden_test, student_outputs, infobench_evaluator)\n",
    "\n",
    "# === Calculate Metrics ===\\\n",
    "metrics = calculate_metrics(results, STUDENT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4829b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RESULTS: test_student\n",
      "==================================================\n",
      "mmlu_med    : 0.8500 (100 examples)\n",
      "graph       : 0.9500 (100 examples)\n",
      "infobench   : 0.0000 (100 examples)\n",
      "--------------------------------------------------\n",
      "OVERALL     : 0.6000\n",
      "==================================================\n",
      "\n",
      "Results saved to: ./eval_results_run15/test_student_results_full.jsonl\n",
      "Metrics saved to: ./eval_results_run15/test_student_metrics_full.json\n"
     ]
    }
   ],
   "source": [
    "# === Save Results ===\n",
    "results_path = os.path.join(OUTPUT_DIR, f\"{STUDENT_ID}_results_full.jsonl\")\n",
    "metrics_path = os.path.join(OUTPUT_DIR, f\"{STUDENT_ID}_metrics_full.json\")\n",
    "\n",
    "save_jsonl(results, results_path)\n",
    "save_json(metrics, metrics_path)\n",
    "\n",
    "# === Print Summary ===\n",
    "print_metrics(metrics)\n",
    "print(f\"\\nResults saved to: {results_path}\")\n",
    "print(f\"Metrics saved to: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f317e262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmi-final-proj-yusenh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
